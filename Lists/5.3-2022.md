# <p align=center> ü¶Ö2022 </p>

<img src="https://img.shields.io/badge/Number%20of%20Papers-69-B55D4C?style=plastic&logo=data:image/svg%2bxml;base64,PHN2ZyBpZD0iTGF5ZXJfMSIgZW5hYmxlLWJhY2tncm91bmQ9Im5ldyAwIDAgNTEwIDUxMCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCA1MTAgNTEwIiB3aWR0aD0iNTEyIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxnPjxnPjxwYXRoIGQ9Im0xNzQuNjEgMzAwYy0yMC41OCAwLTQwLjU2IDYuOTUtNTYuNjkgMTkuNzJsLTExMC4wOSA4NS43OTd2MTA0LjQ4M2g1My41MjlsNzYuNDcxLTY1aDEyNi44MnYtMTQ1eiIgZmlsbD0iI2ZmZGRjZSIvPjwvZz48cGF0aCBkPSJtNTAyLjE3IDI4NC43MmMwIDguOTUtMy42IDE3Ljg5LTEwLjc4IDI0LjQ2bC0xNDguNTYgMTM1LjgyaC03OC4xOHYtODVoNjguMThsMTE0LjM0LTEwMC4yMWMxMi44Mi0xMS4yMyAzMi4wNi0xMC45MiA0NC41LjczIDcgNi41NSAxMC41IDE1LjM4IDEwLjUgMjQuMnoiIGZpbGw9IiNmZmNjYmQiLz48cGF0aCBkPSJtMzMyLjgzIDM0OS42M3YxMC4zN2gtNjguMTh2LTYwaDE4LjU1YzI3LjQxIDAgNDkuNjMgMjIuMjIgNDkuNjMgNDkuNjN6IiBmaWxsPSIjZmZjY2JkIi8+PHBhdGggZD0ibTM5OS44IDc3LjN2OC4wMWMwIDIwLjY1LTguMDQgNDAuMDctMjIuNjQgNTQuNjdsLTExMi41MSAxMTIuNTF2LTIyNi42NmwzLjE4LTMuMTljMTQuNi0xNC42IDM0LjAyLTIyLjY0IDU0LjY3LTIyLjY0IDQyLjYyIDAgNzcuMyAzNC42OCA3Ny4zIDc3LjN6IiBmaWxsPSIjZDAwMDUwIi8+PHBhdGggZD0ibTI2NC42NSAyNS44M3YyMjYuNjZsLTExMi41MS0xMTIuNTFjLTE0LjYtMTQuNi0yMi42NC0zNC4wMi0yMi42NC01NC42N3YtOC4wMWMwLTQyLjYyIDM0LjY4LTc3LjMgNzcuMy03Ny4zIDIwLjY1IDAgNDAuMDYgOC4wNCA1NC42NiAyMi42NHoiIGZpbGw9IiNmZjRhNGEiLz48cGF0aCBkPSJtMjEyLjgzIDM2MC4xMnYzMGg1MS44MnYtMzB6IiBmaWxsPSIjZmZjY2JkIi8+PHBhdGggZD0ibTI2NC42NSAzNjAuMTJ2MzBoMzYuMTRsMzIuMDQtMzB6IiBmaWxsPSIjZmZiZGE5Ii8+PC9nPjwvc3ZnPg==" alt="PaperNum"/>

## Details

* (arXiv preprint 2022) [üí¨Spatial Relationship] **Benchmarking Spatial Relationships in Text-to-Image Generation**, Tejas Gokhale et al. [[Paper](https://arxiv.org/abs/2212.10015)]
* (arXiv preprint 2022) [üí¨Multilingual Communication System] **Multilingual Communication System with Deaf Individuals Utilizing Natural and Visual Languages**, Tuan-Luc Huynh et al.  [[Paper](https://arxiv.org/abs/2212.00305v1)]
* (arXiv preprint 2022) **ReCo: Region-Controlled Text-to-Image Generation**, Zhengyuan Yang et al.  [[Paper](https://arxiv.org/abs/2211.15518)]
* (arXiv preprint 2022) [üí¨Human Evaluation] **Human Evaluation of Text-to-Image Models on a Multi-Task Benchmark**, Vitali Petsiuk et al.  [[Paper](https://arxiv.org/abs/2211.12112)]
* (arXiv preprint 2022) **HumanDiffusion: a Coarse-to-Fine Alignment Diffusion Framework for Controllable Text-Driven Person Image Generation**, Kaiduo Zhang et al.  [[Paper](https://arxiv.org/abs/2211.06235)]
* (arXiv preprint 2022) **eDiffi: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers**, Yogesh Balaji et al.  [[Paper](https://arxiv.org/abs/2211.01324)] [[Project](https://deepimagination.cc/eDiffi/)] [[Video](https://www.youtube.com/watch?v=k6cOx9YjHJc)]
* (arXiv preprint 2022) [üí¨Text-Image Consistency] **Towards Better Text-Image Consistency in Text-to-Image Generation**, Zhaorui Tan et al.  [[Paper](https://arxiv.org/abs/2210.15235)] 
* (arXiv preprint 2022) **ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts**, Zhida Feng et al.  [[Paper](https://arxiv.org/abs/2210.15257)]
* ‚≠ê(ECCV 2022) [üí¨Evaluation Metrics] **TISE: Bag of Metrics for Text-to-Image Synthesis Evaluation**, Tan M. Dinh et al.  [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960585.pdf)] [[Code](https://github.com/VinAIResearch/tise-toolbox)] [[Project](https://di-mi-ta.github.io/tise/)]
* (ECCV 2022) [üí¨Trace+Text‚ÜíImage] **Trace Controlled Text to Image Generation**, Kun Yan et al.  [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960058.pdf)]
* (arXiv preprint 2022) [üí¨Markup‚ÜíImage] **Markup-to-Image Diffusion Models with Scheduled Sampling**, Yuntian Deng et al.  [[Paper](https://arxiv.org/abs/2210.05147)] [[Code](https://github.com/da03/markup2im)]
* (arXiv preprint 2022) **Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation**, Ruijun Li et al.  [[Paper](https://arxiv.org/abs/2210.09549)]
* ‚≠ê(arXiv preprint 2022) **DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics**, Ivan Kapelyukh et al.  [[Paper](https://arxiv.org/abs/2210.02438v1)] [[Project](https://www.robot-learning.uk/dall-e-bot)]
* (arXiv preprint 2022) **Progressive Denoising Model for Fine-Grained Text-to-Image Generation**, Zhengcong Fei et al.  [[Paper](https://arxiv.org/abs/2210.02291v1)] 
* (arXiv preprint 2022) **Creative Painting with Latent Diffusion Models**, Xianchao Wu  [[Paper](https://arxiv.org/abs/2209.14697)] 
* (arXiv preprint 2022) **Re-Imagen: Retrieval-Augmented Text-to-Image Generator**, Wenhu Chen et al.  [[Paper](https://arxiv.org/abs/2209.14491)] 
* (ACMMM 2022) **AtHom: Two Divergent Attentions Stimulated By Homomorphic Training in Text-to-Image Synthesis**, Zhenbo Shi et al. [[Paper](https://dl.acm.org/doi/abs/10.1145/3503161.3548159)]
* (ACMMM 2022) **Adma-GAN: Attribute-Driven Memory Augmented GANs for Text-to-Image Generation**, Xintian Wu et al.  [[Paper](https://arxiv.org/abs/2209.14046)] [[Code](https://github.com/Hsintien-Ng/Adma-GAN)]
* (ACMMM 2022) **AI Illustrator: Translating Raw Descriptions into Images by Prompt-based Cross-Modal Generation**, Yiyang Ma et al.  [[Paper](https://arxiv.org/abs/2209.03160v1)] 
* (ACMMM 2022) **DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation**, Mengqi Huang et al.  [[Paper](https://dl.acm.org/doi/10.1145/3503161.3547881)] 
* (arXiv preprint 2022) [üí¨Radiology] **What Does DALL-E 2 Know About Radiology?**, Lisa C. Adams et al.  [[Paper](https://arxiv.org/abs/2209.13696)] 
* (arXiv preprint 2022) **Prompt-to-Prompt Image Editing with Cross Attention Control**, Amir Hertz et al.  [[Paper](https://arxiv.org/abs/2208.01626)] [[Code](https://github.com/google/prompt-to-prompt)] [[Unofficial Code](https://github.com/bloc97/CrossAttentionControl)] [[Project](https://prompt-to-prompt.github.io/)]
* (arXiv preprint 2022) **Text to Image Generation: Leaving no Language Behind**, Pedro Reviriego et al.  [[Paper](https://arxiv.org/abs/2208.09333)] 
* (arXiv preprint 2022) **Vision-Language Matching for Text-to-Image Synthesis via Generative Adversarial Networks**, Qingrong Cheng et al.  [[Paper](https://arxiv.org/abs/2208.09596)] 
* (arXiv preprint 2022) [üí¨Visual Understanding on Generated Images] **How good are deep models in understanding the generated images?**, Ali Borji  [[Paper](https://arxiv.org/abs/2208.10760)] 
* (arXiv preprint 2022) **Text-to-Image Generation via Implicit Visual Guidance and Hypernetwork**, Xin Yuan et al.  [[Paper](https://arxiv.org/abs/2208.08493)] 
* (arXiv preprint 2022) [üí¨Hybrid word‚ÜíImage] **Adversarial Attacks on Image Generation With Made-Up Words**, Rapha√´l Milli√®re  [[Paper](https://arxiv.org/abs/2208.04135)] 
* (arXiv preprint 2022) **Memory-Driven Text-to-Image Generation**, Bowen Li et al.  [[Paper](https://arxiv.org/abs/2208.07022)] 
* (arXiv preprint 2022) [üí¨Text-to-Person]**T-Person-GAN: Text-to-Person Image Generation with Identity-Consistency and Manifold Mix-Up**, Lin Wu et al.  [[Paper](https://arxiv.org/abs/2208.12752)] [[Code](https://github.com/linwu-github/Person-Image-Generation)]
* (arXiv preprint 2022) **LogicRank: Logic Induced Reranking for Generative Text-to-Image Systems**, Bj√∂rn Deiseroth et al.  [[Paper](https://arxiv.org/abs/2208.13518)] 
* (arXiv preprint 2022) [üí¨Text‚ÜíLayout‚ÜíImage] **Layout-Bridging Text-to-Image Synthesis**, Jiadong Liang et al.  [[Paper](https://arxiv.org/abs/2208.06162)] 
* (arXiv preprint 2022) **DALLE-URBAN: Capturing the urban design expertise of large text to image transformers**, Sachith Seneviratne et al.  [[Paper](https://arxiv.org/abs/2208.04139)] [[Generated Images](https://github.com/sachith500/DALLEURBAN)] 
* (arXiv preprint 2022) **An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion**, Rinon Gal et al.  [[Paper](https://arxiv.org/abs/2208.01618)] [[Code](https://github.com/rinongal/textual_inversion)] [[Project](https://textual-inversion.github.io/)]
* (arXiv preprint 2022) [üí¨Relational Understanding Analysis] **Testing Relational Understanding in Text-Guided Image Generation**, Colin Conwell  [[Paper](https://arxiv.org/abs/2208.00005)] 
* (arXiv preprint 2022) [üí¨Lighting Consistency Analysis] **Lighting (In)consistency of Paint by Text**, Hany Farid  [[Paper](https://arxiv.org/abs/2207.13744)] 
* (ECCV 2022) **Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors**, Oran Gafni et al. [[Paper](https://arxiv.org/abs/2203.13131)] [[Code](https://github.com/CasualGANPapers/Make-A-Scene)] [[The Little Red Boat Story](https://www.youtube.com/watch?v=N4BagnXzPXY)]
* (ECCV 2022) **Compositional Visual Generation with Composable Diffusion Models**, Nan Liu et al. [[Paper](https://arxiv.org/abs/2206.01714)] [[Code](https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch)] [[Project](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/)] [[Hugging Face](https://huggingface.co/spaces/Shuang59/Composable-Diffusion)]
* (arXiv preprint 2022) **Exploring Generative Adversarial Networks for Text-to-Image Generation with Evolution Strategies**, Victor Costa et al.  [[Paper](https://arxiv.org/abs/2207.02907)] 
* (CVPR 2022) **Text to Image Generation with Semantic-Spatial Aware GAN**, Wentong Liao et al. [[Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Liao_Text_to_Image_Generation_With_Semantic-Spatial_Aware_GAN_CVPR_2022_paper.pdf)] [[Code](https://github.com/wtliao/text2image)]
* (ICMR 2022) **Disentangled Representations and Hierarchical Refinement of Multi-Granularity Features for Text-to-Image Synthesis**, Pei Dong et al.  [[Paper](https://dl.acm.org/doi/abs/10.1145/3512527.3531389)] 
* (arXiv preprint 2022) **Scaling Autoregressive Models for Content-Rich Text-to-Image Generation**, Jiahui Yu et al.  [[Paper](https://arxiv.org/abs/2206.10789)] [[Code](https://github.com/google-research/parti)] [[Project](https://parti.research.google/)]
* (Information Sciences-2022) **Text-to-Image Synthesis: Starting Composite from the Foreground Content**, Zhiqiang Zhang et al. [[Paper](https://www.sciencedirect.com/science/article/pii/S0020025522006399)]
* (Applied Intelligence-2022) **Generative adversarial network based on semantic consistency for text-to-image generation**, Yue Ma et al. [[Paper](https://link.springer.com/article/10.1007/s10489-022-03660-8)]
* (ICML 2022) **GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models**, Alex Nichol et al. [[Paper](https://arxiv.org/abs/2112.10741)] [[Code](https://github.com/openai/glide-text2im)]
* (SIGGRAPH 2022) **Text2Human: Text-Driven Controllable Human Image Generation**, Yuming Jiang et al. [[Paper](https://arxiv.org/pdf/2205.15996.pdf)] [[Code](https://github.com/yumingj/Text2Human)]
* ‚≠ê(arXiv preprint 2022) [Imagen] **Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding**, Chitwan Saharia et al. [[Paper](https://arxiv.org/abs/2205.11487)] [[Blog](https://gweb-research-imagen.appspot.com/)]
* (ICME 2022) **GR-GAN: Gradual Refinement Text-to-image Generation**, Bo Yang et al. [[Paper](https://arxiv.org/abs/2205.11273)] [[Code](https://github.com/BoO-18/GR-GAN)]
* (CHI 2022) **Design Guidelines for Prompt Engineering Text-to-Image Generative Models**, Vivian Liu et al. [[Paper](https://dl.acm.org/doi/10.1145/3491102.3501825)]
* (Neural Processing Letters-2022) **PBGN: Phased Bidirectional Generation Network in Text-to-Image Synthesis**, Jianwei Zhu et al. [[Paper](https://link.springer.com/article/10.1007/s11063-022-10866-x)]
* (Signal Processing: Image Communication-2022) **ARRPNGAN: Text-to-image GAN with attention regularization and region proposal networks**, Fengnan Quan et al. [[Paper](https://www.sciencedirect.com/science/article/pii/S0923596522000601)] [[Code](https://github.com/quanFN/ARRPNGAN)]
* (arXiv preprint 2022) **CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers**, Ming Ding et al. [[Paper](https://arxiv.org/abs/2204.14217)] [[Code](https://github.com/THUDM/CogView2)]
* ‚≠ê(OpenAI) [DALL-E 2] **Hierarchical Text-Conditional Image Generation with CLIP Latents**, Aditya Ramesh et al. [[Paper](https://cdn.openai.com/papers/dall-e-2.pdf)] [[Blog](https://openai.com/dall-e-2/)] [[Risks and Limitations](https://github.com/openai/dalle-2-preview/blob/main/system-card.md)] [[Unofficial Code](https://github.com/lucidrains/DALLE2-pytorch)] 
* (arXiv preprint 2022) **Recurrent Affine Transformation for Text-to-image Synthesis**, Senmao Ye et al. [[Paper](https://arxiv.org/abs/2204.10482)] [[Code](https://github.com/senmaoy/Recurrent-Affine-Transformation-for-Text-to-image-Synthesis)]
* (AAAI 2022) **Interactive Image Generation with Natural-Language Feedback**, Yufan Zhou et al. [[Paper](https://www.aaai.org/AAAI22Papers/AAAI-7081.ZhouY.pdf)]
* (IEEE Transactions on Neural Networks and Learning Systems-2022) **DR-GAN: Distribution Regularization for Text-to-Image Generation**, Hongchen Tan et al. [[Paper](https://arxiv.org/abs/2204.07945)] [[Code](https://github.com/Tan-H-C/DR-GAN-Distribution-Regularization-for-Text-to-Image-Generation)]
* (Pattern Recognition Letters-2022) **Text-to-image synthesis with self-supervised learning**, Yong Xuan Tan et al. [[Paper](https://www.sciencedirect.com/science/article/pii/S0167865522001064)] 
* (CVPR 2022) **Vector Quantized Diffusion Model for Text-to-Image Synthesis**, Shuyang Gu et al. [[Paper](https://arxiv.org/abs/2111.14822)] [[Code](https://github.com/microsoft/vq-diffusion)]
* (CVPR 2022) **Autoregressive Image Generation using Residual Quantization**, Doyup Lee et al. [[Paper](https://arxiv.org/abs/2203.01941)] [[Code](https://github.com/kakaobrain/rq-vae-transformer)] 
* (CVPR 2022) **Text-to-Image Synthesis based on Object-Guided Joint-Decoding Transformer**, Fuxiang Wu et al. [[Paper](https://fengxianghe.github.io/paper/wu2022text.pdf)]
* (CVPR 2022) **LAFITE: Towards Language-Free Training for Text-to-Image Generation**, Yufan Zhou et al. [[Paper](https://arxiv.org/abs/2111.13792)] [[Code](https://github.com/drboog/Lafite)] 
* (CVPR 2022) **DF-GAN:  A Simple and Effective Baseline for Text-to-Image Synthesis**, Ming Tao et al. [[Paper](https://arxiv.org/abs/2008.05865)] [[Code](https://github.com/tobran/DF-GAN)] 
* (arXiv preprint 2022) **DT2I: Dense Text-to-Image Generation from Region Descriptions**, Stanislav Frolov et al. [[Paper](https://arxiv.org/abs/2204.02035)] 
* (arXiv preprint 2022) **CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP**, Zihao Wang et al. [[Paper](https://arxiv.org/abs/2203.00386)] [[Code](https://github.com/HFAiLab/clip-gen)]
* (arXiv preprint 2022) **OptGAN: Optimizing and Interpreting the Latent Space of the Conditional Text-to-Image GANs**, Zhenxing Zhang et al. [[Paper](https://arxiv.org/abs/2202.12929)]
* (arXiv preprint 2022) **DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers**, Jaemin Cho et al. [[Paper](https://arxiv.org/abs/2202.04053)] [[Code](https://github.com/j-min/DallEval)] 
* (IEEE Transactions on Network Science and Engineering-2022) **Neural Architecture Search with a Lightweight Transformer for Text-to-Image Synthesis**, Wei Li et al. [[Paper](https://ieeexplore.ieee.org/abstract/document/9699403)] 
* (Neurocomputing-2022) **DiverGAN: An Efficient and Effective Single-Stage Framework for Diverse Text-to-Image Generation**, Zhenxing Zhang et al. [[Paper](https://www.sciencedirect.com/science/article/pii/S0925231221018397)]
* (Knowledge-Based Systems-2022) **CJE-TIG: Zero-shot cross-lingual text-to-image generation by Corpora-based Joint Encoding**, Han Zhang et al. [[Paper](https://www.sciencedirect.com/science/article/pii/S0950705121011138)] 
* (WACV 2022) **StyleMC: Multi-Channel Based Fast Text-Guided Image Generationand Manipulation**, Umut Kocasarƒ± et al. [[Paper](https://arxiv.org/abs/2112.08493)] [[Project](https://catlab-team.github.io/stylemc/)]